## 定义

由输入空间到输出空间为如下函数：*f(x)=sign(wx+b)*

称为感知机，其中sign为符号函数。感知机是一种线性分类模型，属于判别模型。

存在某个超平面S: *wx + b = 0*能够将数据集的正实例点和负实例点完全正确地划分到超平面的两侧，即对所有的y<sub>i</sub>=+1的实例i，有wx<sub>i</sub>+b > 0，对所有的y<sub>i</sub>=-1的实例i，有wx<sub>i</sub>+b < 0，则称数据集T为**线性可分数据集（linearly separable data set）**。



## 损失函数

损失函数的一个自然选择是误分类点的总数，但是这样损失函数不是参数*w*和*b*的连续可导函数，不易优化。损失函数的另一个选择是误分类点到超平面*S*的中距离，这是感知机所采用的。显然，损失函数是非负的，如果没有误分类点，损失函数值是0。

## 训练方法

导入基于误分类的损失函数，利用**梯度下降法**对损失函数进行极小化，求得感知机模型。极小化过程中不是一次使M中所有误分类点的梯度下降，而是**一次随机选取一个误分类点使其梯度下降**。

### 损失函数的梯度怎么取？

假设误分类点集合M是固定的，那么损失函数*L(w, b)*的梯度由：

- <sub>w</sub>L(w, b)=-$\sum$ y<sub>i</sub>x<sub>i</sub>
- <sub>b</sub>L(w, b)=-$\sum$ y<sub>i</sub>

随机选取一个误分类点(x<sub>i</sub>, y<sub>i</sub>)，对w, b进行更新：

- w = -w + $\theta$y<sub>i</sub>x<sub>i</sub>
- b = -b + $\theta$y<sub>i</sub>

通过迭代可以期待损失函数*L(w, b)*不断减小，直到为0。

## 如何计算出感知机的参数？


## 随机梯度下降法（stochastic gradient descent）

任意选取一个超平面*w<sub>0</sub>*, *b<sub>0</sub>*，然后用梯度下降法不断地极小化目标函数。


## 误差逆传播算法 Back Propagation Algorithm

BP算法不仅可用于多层前馈神经网络，还可用于其他类型的神经网络，例如训练递归神经网络。但通常说『BP网络』时，一般是指用BP算法训练的多层前馈神经网络。


## Questions

- PLA迭代一定会停下来吗？如果线性不可分怎么办?
    - 如果数据不是线性可分，那么PLA就不会停下来。
- PLA停下来的时候，是否能保证f~g？如果没有停下来，是否有f~g？
- 全局最优 vs. 局部最优？
